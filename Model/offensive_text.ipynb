{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KafIMyIbjV53",
        "outputId": "89b0e393-0010-4fba-d715-60135770d2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"/content/gdrive/MyDrive/train - output.csv.csv\")"
      ],
      "metadata": {
        "id": "sq29Gk_9jsvs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "9BtYEgRJkBAi",
        "outputId": "d83137aa-d46c-4d6a-8076-13543f334b6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Comment  \\\n",
              "0      There's the slight issue of consent with paedo...   \n",
              "1      THIS IS THE ONLY BIBLE PROPHECY LEFT TO BE FUL...   \n",
              "2      Not everyone has the option of a rainbow react...   \n",
              "3                             Happy Pride!â¤ï¸ðŸŒˆðŸŽ‰   \n",
              "4      don't worry about it, you can get treated, if ...   \n",
              "...                                                  ...   \n",
              "11635                       Incredibly naive most likely   \n",
              "11636  Any nation can say we are liberal, democratic ...   \n",
              "11637  Germany and whole Europe should stop accept th...   \n",
              "11638  What about 1.1 billion the UK has given to hel...   \n",
              "11639  I've met many a beautiful soul, the best actua...   \n",
              "\n",
              "       Background offensive   Offensive      Topic Unnamed: 4  \n",
              "0         Acceptable speech  Acceptable  No target       lgbt  \n",
              "1         Acceptable speech  Acceptable  No target       lgbt  \n",
              "2         Acceptable speech  Acceptable  No target       lgbt  \n",
              "3         Acceptable speech  Acceptable  No target       lgbt  \n",
              "4           Other offensive   Offensive  No target       lgbt  \n",
              "...                     ...         ...        ...        ...  \n",
              "11635       Other offensive   Offensive  No target   migrants  \n",
              "11636     Acceptable speech  Acceptable  No target   migrants  \n",
              "11637  Background offensive   Offensive      Topic   migrants  \n",
              "11638  Background offensive   Offensive      Topic   migrants  \n",
              "11639     Acceptable speech  Acceptable  No target   migrants  \n",
              "\n",
              "[11640 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3fb168f-787c-4fef-98e2-58a08c1974ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Background offensive</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There's the slight issue of consent with paedo...</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>lgbt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>THIS IS THE ONLY BIBLE PROPHECY LEFT TO BE FUL...</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>lgbt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not everyone has the option of a rainbow react...</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>lgbt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy Pride!â¤ï¸ðŸŒˆðŸŽ‰</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>lgbt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>don't worry about it, you can get treated, if ...</td>\n",
              "      <td>Other offensive</td>\n",
              "      <td>Offensive</td>\n",
              "      <td>No target</td>\n",
              "      <td>lgbt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11635</th>\n",
              "      <td>Incredibly naive most likely</td>\n",
              "      <td>Other offensive</td>\n",
              "      <td>Offensive</td>\n",
              "      <td>No target</td>\n",
              "      <td>migrants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11636</th>\n",
              "      <td>Any nation can say we are liberal, democratic ...</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>migrants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11637</th>\n",
              "      <td>Germany and whole Europe should stop accept th...</td>\n",
              "      <td>Background offensive</td>\n",
              "      <td>Offensive</td>\n",
              "      <td>Topic</td>\n",
              "      <td>migrants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11638</th>\n",
              "      <td>What about 1.1 billion the UK has given to hel...</td>\n",
              "      <td>Background offensive</td>\n",
              "      <td>Offensive</td>\n",
              "      <td>Topic</td>\n",
              "      <td>migrants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11639</th>\n",
              "      <td>I've met many a beautiful soul, the best actua...</td>\n",
              "      <td>Acceptable speech</td>\n",
              "      <td>Acceptable</td>\n",
              "      <td>No target</td>\n",
              "      <td>migrants</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11640 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3fb168f-787c-4fef-98e2-58a08c1974ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3fb168f-787c-4fef-98e2-58a08c1974ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3fb168f-787c-4fef-98e2-58a08c1974ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop['lgbt']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "a_EVUpbHkDsS",
        "outputId": "d8be0e02-61ff-4f08-c6c4-dfe6473faf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-66aa96113c47>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lgbt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fm-Xn9acn3Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/train - output.csv.csv\")\n",
        "\n",
        "# Preprocessing function to remove extra special symbols\n",
        "def preprocess_text(text):\n",
        "    # Remove extra special symbols except alphanumeric characters and spaces\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply preprocessing to the 'Comment' column\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "# Write the preprocessed dataset to a new CSV file\n",
        "df.to_csv(\"/content/gdrive/MyDrive/preprocessed_train.csv\", index=False)\n",
        "\n",
        "# Print a message to confirm the successful write\n",
        "print(\"Preprocessed dataset saved to preprocessed_train.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWMk1iaukF42",
        "outputId": "7204846e-fdba-4358-fdce-d2d12c115320"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset saved to preprocessed_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download stopwords and lemmatizer data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/preprocessed_train.csv')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove extra special symbols except alphanumeric characters and spaces\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "        # Convert text to lowercase\n",
        "        cleaned_text = cleaned_text.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = cleaned_text.split()\n",
        "        cleaned_words = [word for word in words if word not in stop_words]\n",
        "        cleaned_text = ' '.join(cleaned_words)\n",
        "\n",
        "        # Lemmatize words\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]\n",
        "        cleaned_text = ' '.join(lemmatized_words)\n",
        "\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply preprocessing to the 'Comment' column\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L82PpfItn4QE",
        "outputId": "865aef2d-6994-4af3-8544-948a16bdbbdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Comment Background offensive  \\\n",
            "0  there slight issue consent paedophilia beastia...    Acceptable speech   \n",
            "1  bible prophecy left fulfilled sincerely wish h...    Acceptable speech   \n",
            "2         everyone option rainbow reaction dont wish    Acceptable speech   \n",
            "3                                        happy pride    Acceptable speech   \n",
            "4                        dont worry get treated must      Other offensive   \n",
            "\n",
            "    Offensive      Topic Unnamed: 4  \n",
            "0  Acceptable  No target       lgbt  \n",
            "1  Acceptable  No target       lgbt  \n",
            "2  Acceptable  No target       lgbt  \n",
            "3  Acceptable  No target       lgbt  \n",
            "4   Offensive  No target       lgbt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/train - output.csv.csv')\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df['Comment'], df['Offensive'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Transform the testing data\n",
        "test_vectors = vectorizer.transform(test_data)\n",
        "\n",
        "# Print the vocabulary size\n",
        "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "\n",
        "# Print the shape of the training and testing vectors\n",
        "print(\"Train vectors shape:\", train_vectors.shape)\n",
        "print(\"Test vectors shape:\", test_vectors.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAvh5Nt4oi4n",
        "outputId": "8752a236-508e-48aa-d083-71c27be73ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14137\n",
            "Train vectors shape: (6723, 14137)\n",
            "Test vectors shape: (1681, 14137)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the labels\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_vectors, train_labels_encoded)\n",
        "\n",
        "# Predict on the testing data\n",
        "predictions_encoded = model.predict(test_vectors)\n",
        "\n",
        "# Decode the predicted labels\n",
        "predictions = label_encoder.inverse_transform(predictions_encoded)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(test_vectors, test_labels_encoded)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-q2PDelpL0y",
        "outputId": "d4dfc36a-b64e-4924-d8cf-747ae2b18688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7459845330160618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['Comment']  # Input features (preprocessed text)\n",
        "y = df['Offensive']  # Target variable (offensive or not)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "a6dIvChiAMc2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Replace missing values with empty string\n",
        "X_train = X_train.fillna('')\n",
        "X_test = X_test.fillna('')\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_vectors = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "p6oaHc-wAV6-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(X_train_vectors, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1PHSNnQsAaKp",
        "outputId": "3129c82e-0349-4203-c37f-c7f377d9fd4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = classifier.score(X_test_vectors, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "#tried linear svc but dropped it coz of low accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoJMxeunAfCD",
        "outputId": "084873c1-847f-454a-9e76-273c5da45559"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7422680412371134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#basic pytorch model\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Define the new texts\n",
        "new_texts = [\n",
        "    \"you are insane.\",\n",
        "    \"You are a terrible!\"\n",
        "]\n",
        "\n",
        "# Tokenize and encode the new texts\n",
        "encoded_inputs = tokenizer(new_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Perform inference using the model\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(**encoded_inputs)\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Map predicted labels to offensive/non-offensive\n",
        "label_map = {0: \"not offensive\", 1: \"offensive\"}\n",
        "predicted_labels = [label_map[label.item()] for label in predicted_labels]\n",
        "\n",
        "# Print the predictions\n",
        "for text, prediction in zip(new_texts, predicted_labels):\n",
        "    print(f\"'{text}' is {prediction}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJI2DTguCkfE",
        "outputId": "932d1ad5-2d92-4fb1-cd18-67f071ac4c8f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'you are insane.' is offensive.\n",
            "'You are a terrible!' is offensive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trying out the vader model\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Define the example text\n",
        "example = \"except satanic new world order whores like these idiots that want to get married in a church,, it's all it is,and these idiots are the pawns in the game of destroy our natural world,, total morons,,go home and be a gay as you like behind closed doors,,have a bit of respect for the family unite and our young children's minds eh,, its been funded by the satanic bankers as usual you stupid sheep,,Planned out before most of you gay people were even born,, your 50 to 100 years behind reality,, all done by design,,\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(example)\n",
        "\n",
        "# Perform sentiment analysis using VADER\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(example)\n",
        "\n",
        "# Determine if the text is offensive based on the sentiment scores\n",
        "is_offensive = sentiment_scores['compound'] < 0\n",
        "\n",
        "# Print the result\n",
        "if is_offensive:\n",
        "    print(\"The text is offensive.\")\n",
        "else:\n",
        "    print(\"The text is not offensive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCxAw1V6D1Pu",
        "outputId": "be0c4429-7982-4e32-8e7a-7cae31e35b79"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text is offensive.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final compound scores added to the csv (used vader)\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Instantiate the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize the result dictionary\n",
        "res = {}\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text = str(row['Comment'])  # Convert to string\n",
        "    res[i] = sia.polarity_scores(text)\n",
        "\n",
        "# Convert the result dictionary to a dataframe\n",
        "vaders = pd.DataFrame(res).T\n",
        "\n",
        "# Reset the index of the original dataframe\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Merge the sentiment scores with the original dataframe\n",
        "merged_df = pd.concat([vaders, df], axis=1)\n",
        "\n",
        "# Function to determine offensive category\n",
        "def get_offensive_category(row):\n",
        "    if row['Background offensive'] == 'Background offensive' or row['Offensive'] == 'Offensive':\n",
        "        return 'Offensive'\n",
        "    else:\n",
        "        return 'Acceptable'\n",
        "\n",
        "# Apply offensive category classification\n",
        "merged_df['Offensive Category'] = merged_df.apply(get_offensive_category, axis=1)\n",
        "\n",
        "# Save the merged dataframe to a new CSV file\n",
        "merged_df.to_csv('merged_data.csv', index=False)\n",
        "\n",
        "print(\"Merged dataframe with compound scores has been saved to merged_data.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L176Mn3PIVbj",
        "outputId": "7c5b5490-d858-4d53-9ee2-a614b0067f94"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "100%|██████████| 11640/11640 [00:06<00:00, 1690.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged dataframe with compound scores has been saved to merged_data.csv.\n"
          ]
        }
      ]
    }
  ]
}